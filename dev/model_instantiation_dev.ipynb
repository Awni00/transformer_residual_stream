{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys; sys.path.append('..')\n",
    "from dual_attention_transformer import DualAttnTransformerLM\n",
    "from language_models import TransformerLM\n",
    "import torch\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an argparse.Namespace object with the specified defaults\n",
    "args = argparse.Namespace(\n",
    "    vocab_size=50304,\n",
    "    d_model=768,\n",
    "    n_layers=12,\n",
    "    sa=6,\n",
    "    ra=6,\n",
    "    n_kv_heads=1,\n",
    "    n_relations=None,\n",
    "    rel_activation='identity',\n",
    "    symbol_type='symbolic_attention',\n",
    "    # symbol_type='position_relative',\n",
    "    sym_attn_n_symbols=512,\n",
    "    trainable_symbols=1,\n",
    "    symmetric_rels=0,\n",
    "    dff=None,\n",
    "    activation='gelu',\n",
    "    dropout_rate=0.0,\n",
    "    norm_first=1,\n",
    "    norm_type='layernorm',\n",
    "    max_block_size=1024,\n",
    "    bias=0,\n",
    "    pos_enc_type='RoPE',\n",
    "    max_seq_len=1024,\n",
    "    shared_symbol_retriever=0,\n",
    "    weight_tie_symbol_library=1,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_len = args.max_seq_len\n",
    "\n",
    "vocab_size = args.vocab_size\n",
    "d_model = args.d_model\n",
    "n_layers = args.n_layers\n",
    "sa, ra = args.sa, args.ra\n",
    "dff = args.dff\n",
    "ra_type = 'relational_attention'\n",
    "symmetric_rels = bool(args.symmetric_rels) if args.symmetric_rels in (0,1) else None\n",
    "n_relations = args.n_relations\n",
    "rel_proj_dim = None if n_relations is None else int((d_model / (sa+ra)) * (ra / n_relations))\n",
    "rel_activation = args.rel_activation\n",
    "symbol_type = args.symbol_type\n",
    "trainable_symbols = bool(args.trainable_symbols)\n",
    "sym_attn_n_symbols = args.sym_attn_n_symbols # args.max_block_size # only applicable for symbol_type=sym_attn\n",
    "activation = args.activation\n",
    "dropout_rate = args.dropout_rate\n",
    "norm_first = bool(args.norm_first)\n",
    "norm_type = args.norm_type\n",
    "max_block_size = args.max_block_size\n",
    "bias = bool(args.bias)\n",
    "pos_enc_type = args.pos_enc_type\n",
    "n_kv_heads = args.n_kv_heads\n",
    "\n",
    "ra_kwargs = dict(n_relations=n_relations, rel_activation=rel_activation, rel_proj_dim=rel_proj_dim, n_kv_heads=n_kv_heads) # FIXME\n",
    "sa_kwargs = dict(n_kv_heads=n_kv_heads) # FIXME NOTE: only used for DAT-LM\n",
    "if symbol_type == 'symbolic_attention':\n",
    "    # NOTE: n_heads, n_symbols fixed for now\n",
    "    symbol_retrieval_kwargs = dict(d_model=d_model, n_symbols=sym_attn_n_symbols, n_heads=4, trainable_symbols=trainable_symbols)\n",
    "elif symbol_type == 'position_relative':\n",
    "    symbol_retrieval_kwargs = dict(symbol_dim=d_model, max_rel_pos=max_seq_len)\n",
    "    ra_kwargs['use_relative_positional_symbols'] = True # if using position-relative symbols, need to tell RA module\n",
    "elif ra != 0:\n",
    "    raise ValueError(f'`symbol_type` {symbol_type} not valid')\n",
    "\n",
    "if ra_type == 'relational_attention':\n",
    "    ra_kwargs['symmetric_rels'] = symmetric_rels\n",
    "\n",
    "symbol_retriever_config = dict(shared_symbol_retriever=bool(args.shared_symbol_retriever), weight_tie_symbol_library=bool(args.weight_tie_symbol_library))\n",
    "\n",
    "# if ra=0, use TransformerLM\n",
    "if ra == 0:\n",
    "    model_config = dict(\n",
    "        vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads=sa, dff=dff,\n",
    "        pos_enc_type=pos_enc_type, dropout_rate=dropout_rate, activation=activation, norm_first=norm_first,\n",
    "        max_block_size=max_seq_len, bias=bias, use_flash_attention=True)\n",
    "\n",
    "    # model = TransformerLM(**model_args).to(device)\n",
    "# otherwise, use DualAttnTransformerLM\n",
    "else:\n",
    "    model_config = dict(\n",
    "        vocab_size=vocab_size, d_model=d_model, n_layers=n_layers, n_heads_sa=sa, n_heads_ra=ra, dff=dff,\n",
    "        sa_kwargs=sa_kwargs, ra_kwargs=ra_kwargs, ra_type=ra_type, pos_enc_type=pos_enc_type, activation=activation,\n",
    "        symbol_retrieval=symbol_type, symbol_retrieval_kwargs=symbol_retrieval_kwargs, symbol_retriever_config=symbol_retriever_config,\n",
    "        dropout_rate=dropout_rate, norm_first=norm_first, max_block_size=max_seq_len, bias=bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'n_heads_ra' in model_config:\n",
    "    model = DualAttnTransformerLM(**model_config)\n",
    "else:\n",
    "    model = TransformerLM(**model_config)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DualAttnTransformerLM                                   --\n",
       "├─ModuleDict: 1-1                                       --\n",
       "│    └─Embedding: 2-1                                   38,633,472\n",
       "│    └─Dropout: 2-2                                     --\n",
       "│    └─ModuleList: 2-3                                  --\n",
       "│    │    └─SymbolicAttention: 3-1                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-2                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-3                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-4                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-5                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-6                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-7                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-8                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-9                      1,377,024\n",
       "│    │    └─SymbolicAttention: 3-10                     1,377,024\n",
       "│    │    └─SymbolicAttention: 3-11                     1,377,024\n",
       "│    │    └─SymbolicAttention: 3-12                     1,377,024\n",
       "│    └─ModuleList: 2-4                                  --\n",
       "│    │    └─DualAttnEncoderBlock: 3-13                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-14                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-15                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-16                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-17                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-18                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-19                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-20                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-21                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-22                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-23                  6,398,976\n",
       "│    │    └─DualAttnEncoderBlock: 3-24                  6,398,976\n",
       "│    └─LayerNorm: 2-5                                   1,536\n",
       "│    └─Linear: 2-6                                      38,633,472\n",
       "================================================================================\n",
       "Total params: 170,580,480\n",
       "Trainable params: 170,580,480\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127,621,632\n"
     ]
    }
   ],
   "source": [
    "print(f'{model.get_num_params():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,198,912\n"
     ]
    }
   ],
   "source": [
    "print(f'{sum(p.numel() for p in model.layers.symbol_retrievers.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127,621,632\n"
     ]
    }
   ],
   "source": [
    "print(f'{sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchinfo.model_statistics.ModelStatistics"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (MB): 0.004176\n",
      "Params size (MB): 665.662464\n",
      "Forward/backward pass size  (MB): 1406.140416\n",
      "Estimated total size (MB): 2071.807056\n",
      "Total Mult-Adds: 172912128\n",
      "trainable_params: 167033088\n",
      "total_params: 167622912\n"
     ]
    }
   ],
   "source": [
    "model_summary_dict = {\n",
    "    'Input size (MB)': model_summary.to_megabytes(model_summary.total_input),\n",
    "    'Params size (MB)': model_summary.to_megabytes(model_summary.total_param_bytes),\n",
    "    'Forward/backward pass size  (MB)': model_summary.to_megabytes(model_summary.total_output_bytes),\n",
    "    'Estimated total size (MB)': model_summary.to_megabytes(model_summary.total_output_bytes + model_summary.total_param_bytes + model_summary.total_input),\n",
    "    'Total Mult-Adds': model_summary.total_mult_adds,\n",
    "\n",
    "    'trainable_params': model_summary.trainable_params,\n",
    "    'total_params': model_summary.total_params,\n",
    "}\n",
    "\n",
    "for k,v in model_summary_dict.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'float_to_megabytes', 'format_output_num', 'formatting', 'input_size', 'summary_list', 'to_megabytes', 'to_readable', 'total_input', 'total_mult_adds', 'total_output_bytes', 'total_param_bytes', 'total_params', 'trainable_params']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4176"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_summary.total_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
