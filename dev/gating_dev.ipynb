{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ResidualGate(nn.Module):\n",
    "    def __init__(self,\n",
    "            d_model: int,\n",
    "            gate_application: str = 'reset-update', # 'reset-update' or 'reset' or 'update' or 'combined' or 'none'\n",
    "            gate_compute: str = 'linear-bias', # 'linear-bias' or 'linear' or 'bias'\n",
    "            gate_activation: str = 'sigmoid', # 'sigmoid' or 'tanh' or 'none\n",
    "            ):\n",
    "        super(ResidualGate, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.gate_application = gate_application\n",
    "        self.gate_compute = gate_compute\n",
    "        self.gate_activation = gate_activation\n",
    "        self.gate_activation_fn = nn.Sigmoid() if gate_activation == 'sigmoid' else nn.Tanh() if gate_activation == 'tanh' else None\n",
    "\n",
    "        bias = gate_compute == 'linear-bias'\n",
    "\n",
    "        if gate_compute in ['linear-bias', 'linear']:\n",
    "            if gate_application == 'reset-update':\n",
    "                self.update_gate_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "                self.reset_gate_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "            elif gate_application == 'reset' or gate_application == 'combined':\n",
    "                self.reset_gate_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "            elif gate_application == 'update':\n",
    "                self.update_gate_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "            elif gate_application == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f'Unknown gate_application: {gate_application}')\n",
    "\n",
    "        elif gate_compute == 'bias':\n",
    "            if gate_application == 'reset-update':\n",
    "                self.update_gate_bias = nn.Parameter(torch.zeros(d_model))\n",
    "                self.reset_gate_bias = nn.Parameter(torch.zeros(d_model))\n",
    "            elif gate_application == 'reset' or gate_application == 'combined':\n",
    "                self.reset_gate_bias = nn.Parameter(torch.zeros(d_model))\n",
    "                # in the combined case, the reset gate is used to compute g*x + (1-g)*y\n",
    "            elif gate_application == 'update':\n",
    "                self.update_gate_bias = nn.Parameter(torch.zeros(d_model))\n",
    "            elif gate_application == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f'Unknown gate_application: {gate_application}')\n",
    "\n",
    "    # TODO: bias initialization (non-zero)\n",
    "\n",
    "    def _compute_update_gate(self, x):\n",
    "        if self.gate_compute in ('linear', 'linear-bias'):\n",
    "            update_gate = self.gate_activation_fn(self.update_gate_linear(x))\n",
    "        elif self.gate_compute == 'bias':\n",
    "            update_gate = self.gate_activation_fn(torch.zeros_like(x) + self.update_gate_bias)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown gate_compute: {self.gate_compute}')\n",
    "\n",
    "        return update_gate\n",
    "\n",
    "    def _compute_reset_gate(self, x):\n",
    "        if self.gate_compute in ('linear', 'linear-bias'):\n",
    "            reset_gate = self.gate_activation_fn(self.reset_gate_linear(x))\n",
    "        elif self.gate_compute == 'bias':\n",
    "            reset_gate = self.gate_activation_fn(torch.zeros_like(x) + self.reset_gate_bias)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown gate_compute: {self.gate_compute}')\n",
    "\n",
    "        return reset_gate\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        if self.gate_application == 'none':\n",
    "            z = x + y\n",
    "        elif self.gate_application == 'update':\n",
    "            update_gate = self._compute_update_gate(x)\n",
    "            z = update_gate * x + y\n",
    "        elif self.gate_application == 'reset':\n",
    "            reset_gate = self._compute_reset_gate(x)\n",
    "            z = reset_gate * x + y\n",
    "        elif self.gate_application == 'reset-update':\n",
    "            update_gate = self._compute_update_gate(x)\n",
    "            reset_gate = self._compute_reset_gate(x)\n",
    "            z = reset_gate * x + update_gate * y\n",
    "        elif self.gate_application == 'combined':\n",
    "            gate = self._compute_reset_gate(x)\n",
    "            z = gate * x + (1 - gate) * y\n",
    "        else:\n",
    "            raise ValueError(f'Unknown gate_application: {self.gate_application}')\n",
    "\n",
    "        return z\n",
    "\n",
    "# TODO: all the gating mechanisms above are x-dependent but not y-dependent\n",
    "# in LSTM, for e.g., gates are both x and y deppendent. i.e., gate = sigmoid(Wx + Uy + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for gate_application=reset-update, gate_compute=linear-bias:\n",
      "update_gate.shape: torch.Size([1, 10, 64])\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,160\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "├─Linear: 1-3                            [1, 10, 64]               4,160\n",
      "├─Sigmoid: 1-4                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 8,320\n",
      "Trainable params: 8,320\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.05\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=reset-update, gate_compute=linear:\n",
      "update_gate.shape: torch.Size([1, 10, 64])\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,096\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "├─Linear: 1-3                            [1, 10, 64]               4,096\n",
      "├─Sigmoid: 1-4                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 8,192\n",
      "Trainable params: 8,192\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.05\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=reset-update, gate_compute=bias:\n",
      "update_gate.shape: torch.Size([1, 10, 64])\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               128\n",
      "├─Sigmoid: 1-1                           [1, 10, 64]               --\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 128\n",
      "Trainable params: 128\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=reset, gate_compute=linear-bias:\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,160\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=reset, gate_compute=linear:\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,096\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 4,096\n",
      "Trainable params: 4,096\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=reset, gate_compute=bias:\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               64\n",
      "├─Sigmoid: 1-1                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=update, gate_compute=linear-bias:\n",
      "update_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,160\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=update, gate_compute=linear:\n",
      "update_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,096\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 4,096\n",
      "Trainable params: 4,096\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=update, gate_compute=bias:\n",
      "update_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               64\n",
      "├─Sigmoid: 1-1                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=combined, gate_compute=linear-bias:\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,160\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=combined, gate_compute=linear:\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "├─Linear: 1-1                            [1, 10, 64]               4,096\n",
      "├─Sigmoid: 1-2                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 4,096\n",
      "Trainable params: 4,096\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.03\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=combined, gate_compute=bias:\n",
      "reset_gate.shape: torch.Size([1, 10, 64])\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               64\n",
      "├─Sigmoid: 1-1                           [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=none, gate_compute=linear-bias:\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=none, gate_compute=linear:\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "Summary for gate_application=none, gate_compute=bias:\n",
      "z.shape: torch.Size([1, 10, 64])\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResidualGate                             [1, 10, 64]               --\n",
      "==========================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = (1, 10, 64)\n",
    "x, y = torch.randn(input_size), torch.randn(input_size)\n",
    "\n",
    "# Possible values for gate_application and gate_compute\n",
    "gate_application_options = ['reset-update', 'reset', 'update', 'combined', 'none']\n",
    "gate_compute_options = ['linear-bias', 'linear', 'bias']\n",
    "\n",
    "# Iterate over each combination\n",
    "for gate_application in gate_application_options:\n",
    "    for gate_compute in gate_compute_options:\n",
    "        print(f\"Summary for gate_application={gate_application}, gate_compute={gate_compute}:\")\n",
    "        model = ResidualGate(64, gate_application, gate_compute)\n",
    "        # Assuming input size (batch_size, channels, height, width)\n",
    "        print(torchinfo.summary(model, input_data=[x, y]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtorchinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'nn.Module'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_SIZE_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'INPUT_DATA_TYPE | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcol_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'torch.device | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtypes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[torch.dtype] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrow_settings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'int | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'ModelStatistics'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Summarize the given PyTorch model. Summarized information includes:\n",
      "    1) Layer names,\n",
      "    2) input/output shapes,\n",
      "    3) kernel shape,\n",
      "    4) # of parameters,\n",
      "    5) # of operations (Mult-Adds),\n",
      "    6) whether layer is trainable\n",
      "\n",
      "NOTE: If neither input_data or input_size are provided, no forward pass through the\n",
      "network is performed, and the provided model information is limited to layer names.\n",
      "\n",
      "Args:\n",
      "    model (nn.Module):\n",
      "            PyTorch model to summarize. The model should be fully in either train()\n",
      "            or eval() mode. If layers are not all in the same mode, running summary\n",
      "            may have side effects on batchnorm or dropout statistics. If you\n",
      "            encounter an issue with this, please open a GitHub issue.\n",
      "\n",
      "    input_size (Sequence of Sizes):\n",
      "            Shape of input data as a List/Tuple/torch.Size\n",
      "            (dtypes must match model input, default is FloatTensors).\n",
      "            You should include batch size in the tuple.\n",
      "            Default: None\n",
      "\n",
      "    input_data (Sequence of Tensors):\n",
      "            Arguments for the model's forward pass (dtypes inferred).\n",
      "            If the forward() function takes several parameters, pass in a list of\n",
      "            args or a dict of kwargs (if your forward() function takes in a dict\n",
      "            as its only argument, wrap it in a list).\n",
      "            Default: None\n",
      "\n",
      "    batch_dim (int):\n",
      "            Batch_dimension of input data. If batch_dim is None, assume\n",
      "            input_data / input_size contains the batch dimension, which is used\n",
      "            in all calculations. Else, expand all tensors to contain the batch_dim.\n",
      "            Specifying batch_dim can be an runtime optimization, since if batch_dim\n",
      "            is specified, torchinfo uses a batch size of 1 for the forward pass.\n",
      "            Default: None\n",
      "\n",
      "    cache_forward_pass (bool):\n",
      "            If True, cache the run of the forward() function using the model\n",
      "            class name as the key. If the forward pass is an expensive operation,\n",
      "            this can make it easier to modify the formatting of your model\n",
      "            summary, e.g. changing the depth or enabled column types, especially\n",
      "            in Jupyter Notebooks.\n",
      "            WARNING: Modifying the model architecture or input data/input size when\n",
      "            this feature is enabled does not invalidate the cache or re-run the\n",
      "            forward pass, and can cause incorrect summaries as a result.\n",
      "            Default: False\n",
      "\n",
      "    col_names (Iterable[str]):\n",
      "            Specify which columns to show in the output. Currently supported: (\n",
      "                \"input_size\",\n",
      "                \"output_size\",\n",
      "                \"num_params\",\n",
      "                \"params_percent\",\n",
      "                \"kernel_size\",\n",
      "                \"mult_adds\",\n",
      "                \"trainable\",\n",
      "            )\n",
      "            Default: (\"output_size\", \"num_params\")\n",
      "            If input_data / input_size are not provided, only \"num_params\" is used.\n",
      "\n",
      "    col_width (int):\n",
      "            Width of each column.\n",
      "            Default: 25\n",
      "\n",
      "    depth (int):\n",
      "            Depth of nested layers to display (e.g. Sequentials).\n",
      "            Nested layers below this depth will not be displayed in the summary.\n",
      "            Default: 3\n",
      "\n",
      "    device (torch.Device):\n",
      "            Uses this torch device for model and input_data.\n",
      "            If not specified, uses the dtype of input_data if given, or the\n",
      "            parameters of the model. Otherwise, uses the result of\n",
      "            torch.cuda.is_available().\n",
      "            Default: None\n",
      "\n",
      "    dtypes (List[torch.dtype]):\n",
      "            If you use input_size, torchinfo assumes your input uses FloatTensors.\n",
      "            If your model use a different data type, specify that dtype.\n",
      "            For multiple inputs, specify the size of both inputs, and\n",
      "            also specify the types of each parameter here.\n",
      "            Default: None\n",
      "\n",
      "    mode (str)\n",
      "            Either \"train\" or \"eval\", which determines whether we call\n",
      "            model.train() or model.eval() before calling summary().\n",
      "            Default: \"eval\".\n",
      "\n",
      "    row_settings (Iterable[str]):\n",
      "            Specify which features to show in a row. Currently supported: (\n",
      "                \"ascii_only\",\n",
      "                \"depth\",\n",
      "                \"var_names\",\n",
      "            )\n",
      "            Default: (\"depth\",)\n",
      "\n",
      "    verbose (int):\n",
      "            0 (quiet): No output\n",
      "            1 (default): Print model summary\n",
      "            2 (verbose): Show weight and bias layers in full detail\n",
      "            Default: 1\n",
      "            If using a Juypter Notebook or Google Colab, the default is 0.\n",
      "\n",
      "    **kwargs:\n",
      "            Other arguments used in `model.forward` function. Passing *args is no\n",
      "            longer supported.\n",
      "\n",
      "Return:\n",
      "    ModelStatistics object\n",
      "            See torchinfo/model_statistics.py for more information.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torchinfo/torchinfo.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "torchinfo.summary?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
