{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePosition(nn.Module):\n",
    "\n",
    "    def __init__(self, num_units, max_relative_position):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.max_relative_position = max_relative_position\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, length_q, length_k):\n",
    "        range_vec_q = torch.arange(length_q)\n",
    "        range_vec_k = torch.arange(length_k)\n",
    "        distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        final_mat = distance_mat_clipped + self.max_relative_position\n",
    "        final_mat = torch.LongTensor(final_mat).cuda()\n",
    "        embeddings = self.embeddings_table[final_mat].cuda()\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relpos = RelativePosition(32, 10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.max_relative_position = 2\n",
    "\n",
    "        self.relative_position_k = RelativePosition(self.head_dim, self.max_relative_position)\n",
    "        self.relative_position_v = RelativePosition(self.head_dim, self.max_relative_position)\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        batch_size = query.shape[0]\n",
    "        len_k = key.shape[1]\n",
    "        len_q = query.shape[1]\n",
    "        len_v = value.shape[1]\n",
    "\n",
    "        query = self.fc_q(query)\n",
    "        key = self.fc_k(key)\n",
    "        value = self.fc_v(value)\n",
    "\n",
    "        r_q1 = query.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        r_k1 = key.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        attn1 = torch.matmul(r_q1, r_k1.permute(0, 1, 3, 2))\n",
    "\n",
    "        print(f'attn1.shape: {attn1.shape}')\n",
    "\n",
    "        r_q2 = query.permute(1, 0, 2).contiguous().view(len_q, batch_size*self.n_heads, self.head_dim)\n",
    "        print(f'r_q2.shape: {r_q2.shape}')\n",
    "        r_k2 = self.relative_position_k(len_q, len_k)\n",
    "        print(f'r_k2.shape: {r_k2.shape}')\n",
    "        print(f'r_k2.transpose(1, 2).shape: {r_k2.transpose(1, 2).shape}')\n",
    "        attn2 = torch.matmul(r_q2, r_k2.transpose(1, 2)).transpose(0, 1)\n",
    "        print(f'attn2.shape: {attn2.shape}')\n",
    "        attn2 = attn2.contiguous().view(batch_size, self.n_heads, len_q, len_k)\n",
    "        print(f'attn2.shape: {attn2.shape}')\n",
    "        attn = (attn1 + attn2) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attn = self.dropout(torch.softmax(attn, dim = -1))\n",
    "\n",
    "        #attn = [batch size, n heads, query len, key len]\n",
    "        r_v1 = value.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        weight1 = torch.matmul(attn, r_v1)\n",
    "        r_v2 = self.relative_position_v(len_q, len_v)\n",
    "        weight2 = attn.permute(2, 0, 1, 3).contiguous().view(len_q, batch_size*self.n_heads, len_k)\n",
    "        weight2 = torch.matmul(weight2, r_v2)\n",
    "        weight2 = weight2.transpose(0, 1).contiguous().view(batch_size, self.n_heads, len_q, self.head_dim)\n",
    "\n",
    "        x = weight1 + weight2\n",
    "\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        #x = [batch size, query len, hid dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        #x = [batch size, query len, hid dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn1.shape: torch.Size([1, 1, 1024, 1024])\n",
      "r_q2.shape: torch.Size([1024, 1, 1024])\n",
      "r_k2.shape: torch.Size([1024, 1024, 1024])\n",
      "r_k2.transpose(1, 2).shape: torch.Size([1024, 1024, 1024])\n",
      "attn2.shape: torch.Size([1, 1024, 1024])\n",
      "attn2.shape: torch.Size([1, 1, 1024, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MultiHeadAttentionLayer                  [1, 1024, 1024]           --\n",
       "├─Linear: 1-1                            [1, 1024, 1024]           1,049,600\n",
       "├─Linear: 1-2                            [1, 1024, 1024]           1,049,600\n",
       "├─Linear: 1-3                            [1, 1024, 1024]           1,049,600\n",
       "├─RelativePosition: 1-4                  [1024, 1024, 1024]        5,120\n",
       "├─Dropout: 1-5                           [1, 1, 1024, 1024]        --\n",
       "├─RelativePosition: 1-6                  [1024, 1024, 1024]        5,120\n",
       "├─Linear: 1-7                            [1, 1024, 1024]           1,049,600\n",
       "==========================================================================================\n",
       "Total params: 4,208,640\n",
       "Trainable params: 4,208,640\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 4.20\n",
       "==========================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 17213.42\n",
       "Params size (MB): 16.83\n",
       "Estimated Total Size (MB): 17242.84\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "b, t, d = 1, 1024, 1024\n",
    "x = torch.rand(b, t, d).cuda()\n",
    "torchinfo.summary(MultiHeadAttentionLayer(d, 1, 0., torch.device('cuda')).cuda(), input_data=(x, x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttentionLayer(64, 1, 0., torch.device('cuda')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn1.shape: torch.Size([8, 1, 10, 10])\n",
      "r_q2.shape: torch.Size([10, 8, 64])\n",
      "r_k2.shape: torch.Size([10, 10, 64])\n",
      "r_k2.transpose(1, 2).shape: torch.Size([10, 64, 10])\n",
      "attn2.shape: torch.Size([8, 10, 10])\n",
      "attn2.shape: torch.Size([8, 1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0738, -0.4885,  0.2331,  ...,  0.0227,  0.3316,  0.2870],\n",
       "         [ 0.0803, -0.4839,  0.2306,  ...,  0.0069,  0.3582,  0.2634],\n",
       "         [ 0.0820, -0.4757,  0.2313,  ..., -0.0107,  0.3658,  0.2641],\n",
       "         ...,\n",
       "         [ 0.0847, -0.4652,  0.2279,  ..., -0.1148,  0.4093,  0.2623],\n",
       "         [ 0.0886, -0.4624,  0.2303,  ..., -0.1338,  0.4151,  0.2642],\n",
       "         [ 0.0823, -0.4887,  0.2152,  ..., -0.1397,  0.3939,  0.2835]],\n",
       "\n",
       "        [[ 0.1091, -0.4367,  0.1713,  ...,  0.0523,  0.3382,  0.2279],\n",
       "         [ 0.1169, -0.4242,  0.1681,  ...,  0.0371,  0.3630,  0.2041],\n",
       "         [ 0.1197, -0.4176,  0.1696,  ...,  0.0187,  0.3759,  0.2026],\n",
       "         ...,\n",
       "         [ 0.1230, -0.4056,  0.1681,  ..., -0.0825,  0.4193,  0.2018],\n",
       "         [ 0.1257, -0.4030,  0.1662,  ..., -0.1053,  0.4261,  0.2082],\n",
       "         [ 0.1219, -0.4294,  0.1560,  ..., -0.1085,  0.4046,  0.2220]],\n",
       "\n",
       "        [[ 0.1571, -0.3615,  0.2784,  ...,  0.0481,  0.3954,  0.2985],\n",
       "         [ 0.1663, -0.3549,  0.2732,  ...,  0.0304,  0.4208,  0.2773],\n",
       "         [ 0.1670, -0.3518,  0.2771,  ...,  0.0149,  0.4255,  0.2787],\n",
       "         ...,\n",
       "         [ 0.1724, -0.3351,  0.2753,  ..., -0.0849,  0.4695,  0.2751],\n",
       "         [ 0.1734, -0.3337,  0.2750,  ..., -0.1071,  0.4801,  0.2775],\n",
       "         [ 0.1682, -0.3645,  0.2592,  ..., -0.1184,  0.4570,  0.2975]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0215, -0.4594,  0.2295,  ...,  0.0569,  0.3981,  0.2010],\n",
       "         [ 0.0302, -0.4574,  0.2249,  ...,  0.0448,  0.4207,  0.1800],\n",
       "         [ 0.0344, -0.4553,  0.2266,  ...,  0.0245,  0.4292,  0.1740],\n",
       "         ...,\n",
       "         [ 0.0351, -0.4354,  0.2254,  ..., -0.0778,  0.4729,  0.1813],\n",
       "         [ 0.0403, -0.4356,  0.2206,  ..., -0.0906,  0.4838,  0.1746],\n",
       "         [ 0.0355, -0.4657,  0.2134,  ..., -0.1028,  0.4582,  0.1976]],\n",
       "\n",
       "        [[ 0.0745, -0.4083,  0.2760,  ..., -0.0033,  0.3561,  0.2904],\n",
       "         [ 0.0863, -0.4047,  0.2739,  ..., -0.0184,  0.3828,  0.2659],\n",
       "         [ 0.0856, -0.4013,  0.2749,  ..., -0.0365,  0.3867,  0.2709],\n",
       "         ...,\n",
       "         [ 0.0918, -0.3914,  0.2736,  ..., -0.1359,  0.4302,  0.2716],\n",
       "         [ 0.0909, -0.3842,  0.2736,  ..., -0.1598,  0.4385,  0.2718],\n",
       "         [ 0.0866, -0.4102,  0.2607,  ..., -0.1652,  0.4124,  0.2926]],\n",
       "\n",
       "        [[ 0.0988, -0.4450,  0.2650,  ...,  0.0633,  0.4004,  0.2207],\n",
       "         [ 0.1076, -0.4409,  0.2649,  ...,  0.0491,  0.4252,  0.2011],\n",
       "         [ 0.1081, -0.4367,  0.2643,  ...,  0.0311,  0.4366,  0.1999],\n",
       "         ...,\n",
       "         [ 0.1122, -0.4185,  0.2626,  ..., -0.0716,  0.4765,  0.1992],\n",
       "         [ 0.1153, -0.4197,  0.2617,  ..., -0.0900,  0.4871,  0.1998],\n",
       "         [ 0.1117, -0.4492,  0.2526,  ..., -0.1008,  0.4631,  0.2183]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(8, 10, 64).cuda()\n",
    "mha(x, x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relpos(5, 5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionalEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, max_position=512):\n",
    "        super(RelativePositionalEncoder, self).__init__()\n",
    "        self.max_position = max_position\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_position * 2 + 1, emb_dim))\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, seq_len_q, seq_len_k):\n",
    "        range_vec_q = torch.arange(seq_len_q)\n",
    "        range_vec_k = torch.arange(seq_len_k)\n",
    "        relative_matrix = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        clipped_relative_matrix = torch.clamp(relative_matrix, -self.max_position, self.max_position)\n",
    "        relative_position_matrix = clipped_relative_matrix + self.max_position\n",
    "        embeddings = self.embeddings_table[relative_position_matrix]\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class T5RelativePositionalEncoder(nn.Module):\n",
    "    def __init__(self, num_heads, max_position=512):\n",
    "        super(T5RelativePositionalEncoder, self).__init__()\n",
    "        self.max_position = max_position\n",
    "        self.embeddings_table = nn.Embedding(max_position*max_position, num_heads)\n",
    "\n",
    "    def forward(self, seq_len_q, seq_len_k):\n",
    "        range_vec_q = torch.arange(seq_len_q).cuda()\n",
    "        range_vec_k = torch.arange(seq_len_k).cuda()\n",
    "        relative_position = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "        relative_position_clipped = torch.clamp(relative_position, -self.max_position, self.max_position)\n",
    "        final_mat = relative_position_clipped + self.max_position\n",
    "        embeddings = self.embeddings_table(final_mat)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 32])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RelativePositionalEncoder(32, 10).cuda()(5, 5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T5RelativePositionalEncoder(8, 10).cuda()(5, 5).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
